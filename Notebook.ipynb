{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAVIGILI/OOD-Co-Act/blob/main/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jkzn-9t9ma2A"
      },
      "outputs": [],
      "source": [
        "# Open this in colab. Restart kernel after numpy installation and the after _distutils_hack installation.\n",
        "\n",
        "%pip uninstall -y accelerate\n",
        "\n",
        "%pip install numpy==1.24.3\n",
        "%pip install --quiet gdown\n",
        "\n",
        "# Install OpenMIM tool\n",
        "%pip install -U openmim\n",
        "\n",
        "# Install a compatible PyTorch (for example, 2.0.0+cu118)\n",
        "%pip install torch==2.0.0+cu118 torchvision==0.15.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
        "\n",
        "%pip install torchmetrics\n",
        "\n",
        "# Then install mmcv-full pre-built for that combination:\n",
        "!mim install \"mmcv-full==1.7.2\" -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Lx5AY-nFm5-w"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "!git clone https://github.com/HAVIGILI/OOD-Co-Act.git\n",
        "sys.path.append(\"/content/OOD-Co-Act\")\n",
        "\n",
        "!git clone -b 0.x --depth 1 https://github.com/open-mmlab/mmsegmentation.git\n",
        "%cd mmsegmentation\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgX3168enJrJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from mmseg.apis import init_segmentor, inference_segmentor\n",
        "import mmcv\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from typing import List, Tuple\n",
        "from calibrator import Calibrator\n",
        "from anomalydetector import AnomalyDetector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn-vpq0gnAo2"
      },
      "outputs": [],
      "source": [
        "# Fishyscapes importet below. Keep this commented out\n",
        "\n",
        "# RoadAnomaly21:\n",
        "# !unzip -q /content/dataset_RoadAnomalyTrack.zip -d RoadAnomaly\n",
        "# !unzip -q /content/gtFine_trainvaltest.zip -d gtFine_trainvaltest\n",
        "# !wget -q http://wwwlehre.dhbw-stuttgart.de/~sgehrig/lostAndFoundDataset/leftImg8bit.zip\n",
        "# !unzip -q leftImg8bit.zip -d /content/dataset_root/\n",
        "# !wget -q http://robotics.ethz.ch/~asl-datasets/Fishyscapes/fishyscapes_lostandfound.zip\n",
        "# !unzip -q fishyscapes_lostandfound.zip -d /content/dataset_root/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7CA5JlmJSaso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get fishyscapes ground truth and pictures.\n",
        "\n",
        "LEFTZIP_ID = \"1BihdEUYOCRKpLfo8VvajTbuUtV3RyFtH\"\n",
        "FISHYZIP_ID = \"1aR_qSjuykKWuKvM1dayecfb37cNYfZC3\"\n",
        "\n",
        "# download both\n",
        "!gdown https://drive.google.com/uc?id=$LEFTZIP_ID -O leftImg8bit.zip\n",
        "!gdown https://drive.google.com/uc?id=$FISHYZIP_ID -O fishyscapes.zip\n",
        "\n",
        "# unzip into the exact paths your code expects\n",
        "!unzip -q leftImg8bit.zip -d /content/drive/MyDrive/\n",
        "!unzip -q fishyscapes.zip -d /content/drive/MyDrive/\n",
        "\n",
        "images_root      = \"/content/drive/MyDrive/leftImg8bit\"\n",
        "annotations_root = \"/content/drive/MyDrive/fishyscapes_lostandfound\""
      ],
      "metadata": {
        "id": "57PIOS6W2uMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "svkZv-JmogDK"
      },
      "outputs": [],
      "source": [
        "# Matching images with ground truth for fishyscapes.\n",
        "\n",
        "import os\n",
        "\n",
        "matched_annotations = []\n",
        "matched_images = []\n",
        "annotated_images = [f[5:].replace(\"_labels.png\", \"_leftImg8bit.png\") for f in os.listdir(annotations_root)]\n",
        "ood_annotations_paths = [os.path.join(annotations_root, f) for f in os.listdir(annotations_root)]\n",
        "\n",
        "# Walk through the image directory\n",
        "for dir_path, dir_names, filenames in os.walk(images_root):\n",
        "        for filename in filenames:\n",
        "            if filename in annotated_images:\n",
        "                index = annotated_images.index(filename)\n",
        "                annotation_path = ood_annotations_paths[index]\n",
        "                image_path = os.path.join(dir_path, filename)\n",
        "                matched_annotations.append(annotation_path)\n",
        "                matched_images.append(image_path)\n",
        "print(\"Matched pairs:\")\n",
        "for annotation, image in zip(matched_annotations, matched_images):\n",
        "    print(\"annotation\", annotation, \"matched with\", image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A4yUC_OnBl3"
      },
      "outputs": [],
      "source": [
        "# Initialize the segmentation model (DeepLabV3+ pretrained on Cityscapes)\n",
        "config_file = 'configs/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes.py'\n",
        "checkpoint_file = ('https://download.openmmlab.com/mmsegmentation/v0.5/'\n",
        "                   'deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/'\n",
        "                   'deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth')\n",
        "model = init_segmentor(config_file, checkpoint_file, device='cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gWzNO72LsvDW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1.  Collect training image / GT paths\n",
        "# --------------------------------------------------------------------\n",
        "def cityscapes_paths(img_root: str, gt_root: str) -> Tuple[List[str], List[str]]:\n",
        "    images, gts = [], []\n",
        "    for city in sorted(os.listdir(img_root)):\n",
        "        d_img, d_gt = os.path.join(img_root, city), os.path.join(gt_root, city)\n",
        "        if not os.path.isdir(d_img):\n",
        "            continue\n",
        "        for fn in sorted(os.listdir(d_img)):\n",
        "            if not fn.endswith('_leftImg8bit.png'):\n",
        "                continue\n",
        "            images.append(os.path.join(d_img, fn))\n",
        "            label_fn = fn.replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
        "            gts.append(os.path.join(d_gt, label_fn))\n",
        "    return images, gts\n",
        "\n",
        "\n",
        "img_root = '/content/drive/MyDrive/leftImg8bit_trainvaltest/leftImg8bit/train'\n",
        "gt_root  = '/content/drive/MyDrive/gtFine_trainvaltest/gtFine/train'\n",
        "images, gts = cityscapes_paths(img_root, gt_root)\n",
        "print(f'Found {len(images)} images')\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2.  Bring up the MMSeg model  (one line)\n",
        "# --------------------------------------------------------------------\n",
        "CFG  = 'configs/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes.py'\n",
        "CKPT = ('https://download.openmmlab.com/mmsegmentation/v0.5/'\n",
        "        'deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/'\n",
        "        'deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth')\n",
        "\n",
        "DEV = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "mmseg_model = init_segmentor(CFG, CKPT, device=DEV)   # <-- raw MMSeg model\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3.  Calibrate (or reload previous statistics)\n",
        "# --------------------------------------------------------------------\n",
        "SAVE = '/content/drive/MyDrive/calibrator_state_ground_truth.pt'\n",
        "\n",
        "if os.path.exists(SAVE):\n",
        "    # one-liner restore: builds a *fresh* model internally\n",
        "    calib = Calibrator.load(SAVE, mmseg_cfg=CFG, mmseg_ckpt=CKPT, device=DEV)\n",
        "else:\n",
        "    calib = Calibrator(mmseg_model)    # hand the model in\n",
        "    calib.run(images, gts)             # long pass over the dataset\n",
        "    calib.save(SAVE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYw_BwdSNOHk"
      },
      "outputs": [],
      "source": [
        "# ╔════════════════════════════════════════╗\n",
        "# ║  Grid-search for LINe + Co-activation  ║\n",
        "# ╚════════════════════════════════════════╝\n",
        "\n",
        "import itertools\n",
        "import gc\n",
        "\n",
        "start, end = 0, 100     # Max 100\n",
        "matched_images_short      = matched_images[start:end]\n",
        "matched_annotations_short = matched_annotations[start:end]\n",
        "images_to_be_plotted      = []  # show figures only for the first image if 0, can use 3:7 to plot image 4-8 for example. Empty for no figures\n",
        "\n",
        "# Hyper-parameter lists (edit to sweep more values)\n",
        "activation_clippings            = [100]\n",
        "activation_prunings             = [0]\n",
        "weight_prunings                 = [0]\n",
        "temperatures_line               = [1]\n",
        "\n",
        "inverse_convert_to_ones         = [False, True]\n",
        "binary_or_not                   = [False, True]\n",
        "wt_thresholds                   = [0, 0]\n",
        "wt_u_thresholds                 = [1, 0.1]\n",
        "temperatures_co                 = [0.3, 1]\n",
        "\n",
        "baseline_and_line_blur_ksizes   = [(3, 3)]\n",
        "baseline_and_line_sigmas        = [0]\n",
        "\n",
        "co_blur_sizes                   = [23]\n",
        "co_sigmas                       = [5]\n",
        "max_pools                       = [11]\n",
        "\n",
        "clips                           = [(0, 1000000), (250000, 10000000), (0, 0)]\n",
        "co_weighteds                    = [0]\n",
        "weight_modes                    = ['softmax']\n",
        "use_c_u_ratios                  = [False, True]\n",
        "\n",
        "plot_pixel_acts   = True\n",
        "plot_pair_ratios  = True\n",
        "id_dot, ood_dot   = (850, 1900, 'lime'), (485, 1360, 'red') # Place these manually by choosing coordinates on a ood object and an id object.\n",
        "\n",
        "state = calib.state_dict()\n",
        "\n",
        "\n",
        "grid = {\n",
        "    \"temp_co\":                temperatures_co,\n",
        "    \"activation_clipping\":    activation_clippings,\n",
        "    \"activation_pruning\":     activation_prunings,\n",
        "    \"weight_pruning\":         weight_prunings,\n",
        "    \"wt_pair\":                list(zip(wt_thresholds, wt_u_thresholds)),\n",
        "    \"co_blur_pair\":           list(zip(co_blur_sizes, co_sigmas)),\n",
        "    \"clip_val\":               clips,\n",
        "    \"temp_line\":              temperatures_line,\n",
        "    \"co_weighted\":            co_weighteds,\n",
        "    \"inv2ones\":               inverse_convert_to_ones,\n",
        "    \"bin_or_not\":             binary_or_not,\n",
        "    \"baseline_pair\":          list(zip(baseline_and_line_blur_ksizes, baseline_and_line_sigmas)),\n",
        "    \"max_pool\":               max_pools,\n",
        "    \"weight_mode\":            weight_modes,\n",
        "    \"c_u_ratio\":              use_c_u_ratios,\n",
        "}\n",
        "\n",
        "param_names, param_values = zip(*grid.items())\n",
        "\n",
        "# 4. Tracking best metrics\n",
        "best_ap    =  -1\n",
        "best_fpr   =   1\n",
        "best_auc   =  -1\n",
        "best_tag_ap  = best_tag_fpr = best_tag_auc = None\n",
        "\n",
        "# 5. Iterate over the Cartesian product of all hyper-params\n",
        "for combo in itertools.product(*param_values):\n",
        "    P = dict(zip(param_names, combo))\n",
        "\n",
        "    # unpack zipped pairs\n",
        "    wt_thr,  wt_thr_u      = P[\"wt_pair\"]\n",
        "    co_blur_ksize, co_sigma = P[\"co_blur_pair\"]\n",
        "    baseline_blur, baseline_sigma = P[\"baseline_pair\"]\n",
        "\n",
        "    # create & configure detector\n",
        "    det = AnomalyDetector(matched_images_short,\n",
        "                          matched_annotations_short,\n",
        "                          model,\n",
        "                          state)\n",
        "    det.plot_pixel_activations = plot_pixel_acts\n",
        "    det.plot_pair_ratios       = plot_pair_ratios\n",
        "    det.images_to_be_plotted   = images_to_be_plotted\n",
        "    det.id_dot, det.ood_dot    = id_dot, ood_dot\n",
        "\n",
        "    # set thresholds & params\n",
        "    det.activation_clipping     = P[\"activation_clipping\"]\n",
        "    det.activation_pruning      = P[\"activation_pruning\"]\n",
        "    det.weight_pruning          = P[\"weight_pruning\"]\n",
        "    det.temperature_co          = P[\"temp_co\"]\n",
        "    det.temperature_line        = P[\"temp_line\"]\n",
        "    det.inverse_convert_to_ones = P[\"inv2ones\"]\n",
        "    det.make_feat_binary        = P[\"bin_or_not\"]\n",
        "    det.wt_threshold            = wt_thr\n",
        "    det.wt_u_threshold          = wt_thr_u\n",
        "    det.co_blur_ksize           = co_blur_ksize\n",
        "    det.co_sigma                = co_sigma\n",
        "    det.blur_ksize              = baseline_blur\n",
        "    det.sigma                   = baseline_sigma\n",
        "    det.max_pool                = P[\"max_pool\"]\n",
        "    det.clips                   = P[\"clip_val\"]\n",
        "    det.co_weighted             = P[\"co_weighted\"]\n",
        "    det.weight_mode             = P[\"weight_mode\"]\n",
        "    det.c_u_ratio               = P[\"c_u_ratio\"]\n",
        "\n",
        "    # run inference\n",
        "    det.ood_inference()\n",
        "    IDs, ood_scores = det.get_ood_score_lists()\n",
        "\n",
        "    # evaluate selected methods\n",
        "    methods = {\n",
        "        \"ood_scores_coact_bin\":       \"Co-act count\",\n",
        "        \"ood_scores_coact_wt\":        \"Co-act magnitude\",\n",
        "        \"ood_scores_coact_any\":       \"Co-act any\",\n",
        "        \"ood_scores_nr_activation\":   \"#activations\",\n",
        "    }\n",
        "\n",
        "    # build a reusable run tag\n",
        "    base_tag = \" | \".join(f\"{k}={P[k]}\" for k in param_names)\n",
        "\n",
        "    scores_line = ood_scores[\"ood_scores_line\"]\n",
        "    for method, title in methods.items():\n",
        "        # scores = -ood_scores[method] / scores_line / scores_line / scores_line\n",
        "        scores = (scores - scores.min()) / (scores.ptp() + 1e-9)\n",
        "\n",
        "        run_tag = f\"method | ={method}{base_tag}\"\n",
        "        print(run_tag)\n",
        "\n",
        "        auc, fpr, ap = det.calculate_metrics(IDs, scores,\n",
        "                                             plot_hist=False,\n",
        "                                             title=title)\n",
        "\n",
        "        if ap  > best_ap:  best_ap,  best_tag_ap  = ap,  run_tag\n",
        "        if fpr < best_fpr: best_fpr, best_tag_fpr = fpr, run_tag\n",
        "        if auc  > best_auc: best_auc, best_tag_auc = auc, run_tag\n",
        "\n",
        "    # cleanup GPU memory\n",
        "    del det\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 6. Print summary of best combinations\n",
        "print(\"\\n══════ BEST RESULTS ══════\")\n",
        "print(f\"• Best AP    = {best_ap:.4f}  → {best_tag_ap}\")\n",
        "print(f\"• Best FPR95 = {best_fpr:.4f}  → {best_tag_fpr}\")\n",
        "print(f\"• Best AUROC = {best_auc:.4f}  → {best_tag_auc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC1Pg3Xt_Pt9"
      },
      "outputs": [],
      "source": [
        "# Set true and run if you need to free memory\n",
        "\n",
        "delete = True\n",
        "if delete:\n",
        "    del det\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    import torch\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}