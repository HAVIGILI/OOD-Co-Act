{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAVIGILI/OOD-Co-Act/blob/main/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Jkzn-9t9ma2A",
        "outputId": "4b4220bb-60d3-4cbc-8882-e138b794df15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.11/dist-packages (1.24.3)\n",
            "Requirement already satisfied: openmim in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from openmim) (8.2.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from openmim) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.11/dist-packages (from openmim) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.11/dist-packages (from openmim) (0.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from openmim) (2.2.2)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.11/dist-packages (from openmim) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openmim) (2.28.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from openmim) (13.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from model-index->openmim) (6.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from model-index->openmim) (3.8)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.11/dist-packages (from model-index->openmim) (4.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim) (4.65.2)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (2025.6.15)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.17.0)\n",
            "Requirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\n",
            "Requirement already satisfied: oss2~=2.17.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (2.17.0)\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (24.2)\n",
            "Requirement already satisfied: setuptools~=60.2.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (60.2.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.11/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.11/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Looking in links: https://download.pytorch.org/whl/cu118/torch_stable.html\n",
            "Collecting torch==2.0.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m926.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu118) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu118) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu118) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0+cu118) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.0+cu118)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.0+cu118) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.0+cu118) (2.28.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.0+cu118) (11.2.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0+cu118) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0+cu118)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.0+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.0+cu118) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.0+cu118) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0+cu118) (1.3.0)\n",
            "Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, triton, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires accelerate>=0.21.0, which is not installed.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 torch-2.0.0+cu118 torchvision-0.15.0+cu118 triton-2.0.0\n",
            "Collecting torchmetrics\n",
            "  Using cached torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.24.3)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Using cached lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (60.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=2.0.0->torchmetrics) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=2.0.0->torchmetrics) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Using cached torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.3\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html, https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n",
            "Collecting mmcv-full==1.7.2\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/mmcv_full-1.7.2-cp311-cp311-manylinux1_x86_64.whl (70.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from mmcv-full==1.7.2)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mmcv-full==1.7.2) (1.24.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mmcv-full==1.7.2) (24.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mmcv-full==1.7.2) (11.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from mmcv-full==1.7.2) (6.0.2)\n",
            "Collecting yapf (from mmcv-full==1.7.2)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.11/dist-packages (from mmcv-full==1.7.2) (4.11.0.86)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->mmcv-full==1.7.2) (4.3.8)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, yapf, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.7.2 yapf-0.43.0\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -y accelerate\n",
        "\n",
        "%pip install numpy==1.24.3\n",
        "%pip install --quiet gdown\n",
        "\n",
        "# Install OpenMIM tool\n",
        "%pip install -U openmim\n",
        "\n",
        "# Install a compatible PyTorch (for example, 2.0.0+cu118)\n",
        "%pip install torch==2.0.0+cu118 torchvision==0.15.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
        "\n",
        "%pip install torchmetrics\n",
        "\n",
        "# Then install mmcv-full pre-built for that combination:\n",
        "!mim install \"mmcv-full==1.7.2\" -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "Lx5AY-nFm5-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d98b554-c104-46b1-b53b-e7032acd4eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mmsegmentation'...\n",
            "remote: Enumerating objects: 1405, done.\u001b[K\n",
            "remote: Counting objects: 100% (1405/1405), done.\u001b[K\n",
            "remote: Compressing objects: 100% (829/829), done.\u001b[K\n",
            "remote: Total 1405 (delta 726), reused 892 (delta 556), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1405/1405), 9.06 MiB | 32.54 MiB/s, done.\n",
            "Resolving deltas: 100% (726/726), done.\n",
            "/content/mmsegmentation\n",
            "Obtaining file:///content/mmsegmentation\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mmsegmentation==0.30.0) (3.10.0)\n",
            "Collecting mmcls>=0.20.1 (from mmsegmentation==0.30.0)\n",
            "  Downloading mmcls-0.25.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mmsegmentation==0.30.0) (1.24.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mmsegmentation==0.30.0) (24.2)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from mmsegmentation==0.30.0) (3.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mmsegmentation==0.30.0) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmsegmentation==0.30.0) (2.9.0.post0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->mmsegmentation==0.30.0) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mmsegmentation==0.30.0) (1.17.0)\n",
            "Downloading mmcls-0.25.0-py2.py3-none-any.whl (648 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.8/648.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mmcls, mmsegmentation\n",
            "  Running setup.py develop for mmsegmentation\n",
            "Successfully installed mmcls-0.25.0 mmsegmentation-0.30.0\n"
          ]
        }
      ],
      "source": [
        "!git clone -b 0.x --depth 1 https://github.com/open-mmlab/mmsegmentation.git\n",
        "%cd mmsegmentation\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hgX3168enJrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc155c54-6544-4d47-f432-bf7bc8bb3c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from mmseg.apis import init_segmentor, inference_segmentor\n",
        "import mmcv\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gn-vpq0gnAo2"
      },
      "outputs": [],
      "source": [
        "# Fishyscapes importet below. Keep this commented out\n",
        "\n",
        "# RoadAnomaly21:\n",
        "# !unzip -q /content/dataset_RoadAnomalyTrack.zip -d RoadAnomaly\n",
        "# !unzip -q /content/gtFine_trainvaltest.zip -d gtFine_trainvaltest\n",
        "# !wget -q http://wwwlehre.dhbw-stuttgart.de/~sgehrig/lostAndFoundDataset/leftImg8bit.zip\n",
        "# !unzip -q leftImg8bit.zip -d /content/dataset_root/\n",
        "# !wget -q http://robotics.ethz.ch/~asl-datasets/Fishyscapes/fishyscapes_lostandfound.zip\n",
        "# !unzip -q fishyscapes_lostandfound.zip -d /content/dataset_root/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CA5JlmJSaso",
        "outputId": "dcd96c7d-aec1-4976-927b-ec3555c2a10d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get fishyscapes ground truth and pictures.\n",
        "\n",
        "LEFTZIP_ID = \"1BihdEUYOCRKpLfo8VvajTbuUtV3RyFtH\"\n",
        "FISHYZIP_ID = \"1aR_qSjuykKWuKvM1dayecfb37cNYfZC3\"\n",
        "\n",
        "# download both\n",
        "!gdown https://drive.google.com/uc?id=$LEFTZIP_ID -O leftImg8bit.zip\n",
        "!gdown https://drive.google.com/uc?id=$FISHYZIP_ID -O fishyscapes.zip\n",
        "\n",
        "# unzip into the exact paths your code expects\n",
        "!unzip -q leftImg8bit.zip -d /content/drive/MyDrive/\n",
        "!unzip -q fishyscapes.zip -d /content/drive/MyDrive/\n",
        "\n",
        "images_root      = \"/content/drive/MyDrive/leftImg8bit\"\n",
        "annotations_root = \"/content/drive/MyDrive/fishyscapes_lostandfound\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57PIOS6W2uMY",
        "outputId": "5c5b9813-086d-46d0-d917-6357e3141bbc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1BihdEUYOCRKpLfo8VvajTbuUtV3RyFtH\n",
            "From (redirected): https://drive.google.com/uc?id=1BihdEUYOCRKpLfo8VvajTbuUtV3RyFtH&confirm=t&uuid=c6d34d48-2cc7-4979-980e-2361f7406e7e\n",
            "To: /content/mmsegmentation/leftImg8bit.zip\n",
            "100% 5.80G/5.80G [00:31<00:00, 185MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aR_qSjuykKWuKvM1dayecfb37cNYfZC3\n",
            "To: /content/mmsegmentation/fishyscapes.zip\n",
            "100% 735k/735k [00:00<00:00, 158MB/s]\n",
            "replace /content/drive/MyDrive/leftImg8bit/train/03_Hanns_Klemm_Str_19/03_Hanns_Klemm_Str_19_000003_000100_leftImg8bit.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "replace /content/drive/MyDrive/fishyscapes_lostandfound/0001_04_Maurener_Weg_8_000000_000090_labels.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "svkZv-JmogDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65c7419-8e4d-4fb1-d2ed-463d1991726e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched pairs:\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0095_13_Elly_Beinhorn_Str_000002_000120_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000002_000120_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0088_13_Elly_Beinhorn_Str_000000_000290_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000000_000290_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0092_13_Elly_Beinhorn_Str_000001_000230_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000001_000230_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0089_13_Elly_Beinhorn_Str_000001_000080_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000001_000080_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0096_13_Elly_Beinhorn_Str_000003_000080_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000003_000080_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0085_13_Elly_Beinhorn_Str_000000_000170_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000000_000170_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0090_13_Elly_Beinhorn_Str_000001_000140_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000001_000140_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0099_13_Elly_Beinhorn_Str_000003_000260_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000003_000260_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0098_13_Elly_Beinhorn_Str_000003_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000003_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0086_13_Elly_Beinhorn_Str_000000_000230_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000000_000230_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0093_13_Elly_Beinhorn_Str_000002_000030_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000002_000030_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0097_13_Elly_Beinhorn_Str_000003_000140_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000003_000140_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0094_13_Elly_Beinhorn_Str_000002_000090_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000002_000090_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0091_13_Elly_Beinhorn_Str_000001_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000001_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0083_13_Elly_Beinhorn_Str_000000_000050_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000000_000050_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0084_13_Elly_Beinhorn_Str_000000_000110_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000000_000110_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0087_13_Elly_Beinhorn_Str_000000_000260_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/13_Elly_Beinhorn_Str/13_Elly_Beinhorn_Str_000000_000260_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0051_01_Hanns_Klemm_Str_45_000003_000090_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000003_000090_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0075_01_Hanns_Klemm_Str_45_000011_000170_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000011_000170_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0050_01_Hanns_Klemm_Str_45_000003_000030_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000003_000030_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0082_01_Hanns_Klemm_Str_45_000012_000280_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000012_000280_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0065_01_Hanns_Klemm_Str_45_000008_000240_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000008_000240_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0037_01_Hanns_Klemm_Str_45_000000_000140_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000000_000140_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0067_01_Hanns_Klemm_Str_45_000009_000150_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000009_000150_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0056_01_Hanns_Klemm_Str_45_000004_000180_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000004_000180_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0059_01_Hanns_Klemm_Str_45_000005_000140_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000005_000140_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0049_01_Hanns_Klemm_Str_45_000002_000220_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000002_000220_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0054_01_Hanns_Klemm_Str_45_000003_000210_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000003_000210_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0078_01_Hanns_Klemm_Str_45_000011_000260_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000011_000260_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0045_01_Hanns_Klemm_Str_45_000001_000240_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000001_000240_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0074_01_Hanns_Klemm_Str_45_000011_000110_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000011_000110_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0080_01_Hanns_Klemm_Str_45_000012_000220_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000012_000220_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0048_01_Hanns_Klemm_Str_45_000002_000190_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000002_000190_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0068_01_Hanns_Klemm_Str_45_000009_000210_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000009_000210_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0062_01_Hanns_Klemm_Str_45_000008_000060_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000008_000060_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0061_01_Hanns_Klemm_Str_45_000005_000230_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000005_000230_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0077_01_Hanns_Klemm_Str_45_000011_000230_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000011_000230_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0063_01_Hanns_Klemm_Str_45_000008_000120_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000008_000120_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0073_01_Hanns_Klemm_Str_45_000011_000050_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000011_000050_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0057_01_Hanns_Klemm_Str_45_000004_000210_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000004_000210_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0040_01_Hanns_Klemm_Str_45_000000_000260_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000000_000260_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0060_01_Hanns_Klemm_Str_45_000005_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000005_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0076_01_Hanns_Klemm_Str_45_000011_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000011_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0069_01_Hanns_Klemm_Str_45_000010_000050_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000010_000050_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0038_01_Hanns_Klemm_Str_45_000000_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000000_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0044_01_Hanns_Klemm_Str_45_000001_000210_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000001_000210_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0072_01_Hanns_Klemm_Str_45_000010_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000010_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0070_01_Hanns_Klemm_Str_45_000010_000110_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000010_000110_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0043_01_Hanns_Klemm_Str_45_000001_000180_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000001_000180_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0058_01_Hanns_Klemm_Str_45_000005_000080_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000005_000080_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0047_01_Hanns_Klemm_Str_45_000002_000130_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000002_000130_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0071_01_Hanns_Klemm_Str_45_000010_000170_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000010_000170_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0066_01_Hanns_Klemm_Str_45_000009_000090_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000009_000090_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0041_01_Hanns_Klemm_Str_45_000001_000060_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000001_000060_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0042_01_Hanns_Klemm_Str_45_000001_000120_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000001_000120_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0052_01_Hanns_Klemm_Str_45_000003_000150_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000003_000150_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0046_01_Hanns_Klemm_Str_45_000002_000070_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000002_000070_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0079_01_Hanns_Klemm_Str_45_000012_000160_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000012_000160_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0064_01_Hanns_Klemm_Str_45_000008_000180_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000008_000180_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0036_01_Hanns_Klemm_Str_45_000000_000080_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000000_000080_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0053_01_Hanns_Klemm_Str_45_000003_000180_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000003_000180_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0055_01_Hanns_Klemm_Str_45_000004_000120_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000004_000120_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0039_01_Hanns_Klemm_Str_45_000000_000230_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000000_000230_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0081_01_Hanns_Klemm_Str_45_000012_000250_labels.png matched with /content/drive/MyDrive/leftImg8bit/train/01_Hanns_Klemm_Str_45/01_Hanns_Klemm_Str_45_000012_000250_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0031_04_Maurener_Weg_8_000008_000040_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000008_000040_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0026_04_Maurener_Weg_8_000006_000130_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000006_000130_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0023_04_Maurener_Weg_8_000005_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000005_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0030_04_Maurener_Weg_8_000007_000170_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000007_000170_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0029_04_Maurener_Weg_8_000007_000140_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000007_000140_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0020_04_Maurener_Weg_8_000005_000050_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000005_000050_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0024_04_Maurener_Weg_8_000006_000040_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000006_000040_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0010_04_Maurener_Weg_8_000002_000120_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000002_000120_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0008_04_Maurener_Weg_8_000001_000210_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000001_000210_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0015_04_Maurener_Weg_8_000004_000020_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000004_000020_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0011_04_Maurener_Weg_8_000002_000150_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000002_000150_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0016_04_Maurener_Weg_8_000004_000080_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000004_000080_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0018_04_Maurener_Weg_8_000004_000170_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000004_000170_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0012_04_Maurener_Weg_8_000003_000030_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000003_000030_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0034_04_Maurener_Weg_8_000008_000160_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000008_000160_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0021_04_Maurener_Weg_8_000005_000110_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000005_000110_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0028_04_Maurener_Weg_8_000007_000080_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000007_000080_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0007_04_Maurener_Weg_8_000001_000180_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000001_000180_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0014_04_Maurener_Weg_8_000003_000120_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000003_000120_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0004_04_Maurener_Weg_8_000001_000030_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000001_000030_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0019_04_Maurener_Weg_8_000004_000200_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000004_000200_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0001_04_Maurener_Weg_8_000000_000090_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000000_000090_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0009_04_Maurener_Weg_8_000002_000060_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000002_000060_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0013_04_Maurener_Weg_8_000003_000090_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000003_000090_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0027_04_Maurener_Weg_8_000007_000020_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000007_000020_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0006_04_Maurener_Weg_8_000001_000150_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000001_000150_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0017_04_Maurener_Weg_8_000004_000140_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000004_000140_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0002_04_Maurener_Weg_8_000000_000150_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000000_000150_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0005_04_Maurener_Weg_8_000001_000090_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000001_000090_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0032_04_Maurener_Weg_8_000008_000100_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000008_000100_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0000_04_Maurener_Weg_8_000000_000030_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000000_000030_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0022_04_Maurener_Weg_8_000005_000170_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000005_000170_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0033_04_Maurener_Weg_8_000008_000130_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000008_000130_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0003_04_Maurener_Weg_8_000000_000180_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000000_000180_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0035_04_Maurener_Weg_8_000008_000190_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000008_000190_leftImg8bit.png\n",
            "annotation /content/drive/MyDrive/fishyscapes_lostandfound/0025_04_Maurener_Weg_8_000006_000100_labels.png matched with /content/drive/MyDrive/leftImg8bit/test/04_Maurener_Weg_8/04_Maurener_Weg_8_000006_000100_leftImg8bit.png\n"
          ]
        }
      ],
      "source": [
        "# Matching images with ground truth for fishyscapes.\n",
        "\n",
        "import os\n",
        "\n",
        "matched_annotations = []\n",
        "matched_images = []\n",
        "annotated_images = [f[5:].replace(\"_labels.png\", \"_leftImg8bit.png\") for f in os.listdir(annotations_root)]\n",
        "ood_annotations_paths = [os.path.join(annotations_root, f) for f in os.listdir(annotations_root)]\n",
        "\n",
        "# Walk through the image directory\n",
        "for dir_path, dir_names, filenames in os.walk(images_root):\n",
        "        for filename in filenames:\n",
        "            if filename in annotated_images:\n",
        "                index = annotated_images.index(filename)\n",
        "                annotation_path = ood_annotations_paths[index]\n",
        "                image_path = os.path.join(dir_path, filename)\n",
        "                matched_annotations.append(annotation_path)\n",
        "                matched_images.append(image_path)\n",
        "print(\"Matched pairs:\")\n",
        "for annotation, image in zip(matched_annotations, matched_images):\n",
        "    print(\"annotation\", annotation, \"matched with\", image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1A4yUC_OnBl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc7fed3-c8bf-4c57-c1e0-797dd9aa2f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth\n",
            "100%|██████████| 239M/239M [00:07<00:00, 35.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the segmentation model (DeepLabV3+ pretrained on Cityscapes)\n",
        "config_file = 'configs/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes.py'\n",
        "checkpoint_file = ('https://download.openmmlab.com/mmsegmentation/v0.5/'\n",
        "                   'deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/'\n",
        "                   'deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth')\n",
        "model = init_segmentor(config_file, checkpoint_file, device='cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BUOeu7jZneYE"
      },
      "outputs": [],
      "source": [
        "# calibration.py – Calibrator for LINe and pair-wise co-activation stats\n",
        "# =====================================================================\n",
        "# Author: <your-name>\n",
        "# This module gathers statistics that are later used for OOD detection.\n",
        "# Everything is pure PyTorch, no heavy dependencies besides MMSeg.\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mmseg.apis import inference_segmentor\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "#  A fixed raw-to-train LUT for the Cityscapes labels\n",
        "# ----------------------------------------------------------------------\n",
        "_RAW2TRAIN: dict[int, int] = {\n",
        "    7: 0,  8: 1, 11: 2, 12: 3, 13: 4, 17: 5, 19: 6, 20: 7,\n",
        "    21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
        "    28: 15, 31: 16, 32: 17, 33: 18,\n",
        "}\n",
        "_LUT = np.full(256, 255, np.uint8)          # 255 = “ignore”\n",
        "for raw_id, train_id in _RAW2TRAIN.items():\n",
        "    _LUT[raw_id] = train_id\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "#  Calibrator\n",
        "# ======================================================================\n",
        "class Calibrator:\n",
        "    \"\"\"Collect LINe tensors *and* pair-wise co-activation statistics.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        mmseg_model: nn.Module,            # ← pass a *ready* MMSeg model\n",
        "        device: str | None = None,\n",
        "        n_classes: int = 19,\n",
        "        n_channels: int = 512,\n",
        "    ) -> None:\n",
        "        self.model = mmseg_model\n",
        "        self.dev   = device or next(mmseg_model.parameters()).device\n",
        "        self.K, self.C = n_classes, n_channels\n",
        "\n",
        "        # ---------- LINe tensors ----------\n",
        "        self.contrib = torch.zeros(self.K, self.C, device=self.dev)\n",
        "        self.avg_act = torch.zeros(self.K, self.C, device=self.dev)\n",
        "\n",
        "        # ---------- per-class accumulators ----------\n",
        "        sz = (self.K, self.C, self.C)\n",
        "        self.co_bin_count   = torch.zeros(sz, device=self.dev)\n",
        "        self.co_wt_sum      = torch.zeros(sz, device=self.dev)\n",
        "        self.co_wt_sum_both = torch.zeros(sz, device=self.dev)\n",
        "        self.pixel_counter  = torch.zeros(self.K, device=self.dev)\n",
        "\n",
        "        # ---------- global accumulators ----------\n",
        "        self.co_bin_all     = torch.zeros(self.C, self.C, device=self.dev)\n",
        "        self.co_wt_all      = torch.zeros(self.C, self.C, device=self.dev)\n",
        "        self.co_wt_both_all = torch.zeros(self.C, self.C, device=self.dev)\n",
        "        self.pixel_all      = torch.tensor(0.0, device=self.dev)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #  Main dataset pass\n",
        "    # ------------------------------------------------------------------\n",
        "    def run(self, images: List[str], gt_paths: List[str]) -> None:\n",
        "        \"\"\"Iterate over *images* and *gt_paths* to fill all statistics.\"\"\"\n",
        "\n",
        "        hook_buf: dict[str, torch.Tensor] = {}\n",
        "\n",
        "        def _cache_feat(_, inp, __):\n",
        "            hook_buf[\"feat\"] = inp[0].detach()          # [1,C,Hf,Wf]\n",
        "\n",
        "        self.model.decode_head.conv_seg.register_forward_hook(_cache_feat)\n",
        "        w = self.model.decode_head.conv_seg.weight.data.squeeze(-1).squeeze(-1)  # [K,C]\n",
        "\n",
        "        for idx, (img_p, gt_p) in enumerate(zip(images, gt_paths), 1):\n",
        "            if idx % 25 == 0:\n",
        "                print(f\"[Calibrator] processed {idx}/{len(images)} images …\")\n",
        "\n",
        "            _ = inference_segmentor(self.model, img_p)           # runs model\n",
        "            feat = hook_buf.pop(\"feat\").squeeze(0)               # [C,Hf,Wf]\n",
        "            Hf, Wf = feat.shape[1:]\n",
        "\n",
        "            # ------------------ prepare ground truth ---------------------\n",
        "            gt_raw   = cv2.imread(gt_p, cv2.IMREAD_GRAYSCALE)\n",
        "            gt_train = _LUT[gt_raw]                              # 0-18 / 255\n",
        "            valid    = (gt_train != 255).astype(np.uint8)\n",
        "\n",
        "            gt_t = torch.from_numpy(gt_train)[None, None].float().to(self.dev)\n",
        "            gt_t = F.interpolate(gt_t, size=(Hf, Wf), mode=\"nearest\").long().squeeze()\n",
        "\n",
        "            val_t = torch.from_numpy(valid)[None, None].float().to(self.dev)\n",
        "            val_t = F.interpolate(val_t, size=(Hf, Wf), mode=\"nearest\").bool().squeeze()\n",
        "\n",
        "            # ------------------ accumulate statistics -------------------\n",
        "            for cls in range(self.K):\n",
        "                mask = (gt_t == cls) & val_t\n",
        "                if not mask.any():\n",
        "                    continue\n",
        "\n",
        "                vecs = feat.permute(1, 2, 0)[mask]      # [N,C]\n",
        "                binm = (vecs > 0).float()               # [N,C]\n",
        "                n    = vecs.size(0)\n",
        "\n",
        "                # per-class\n",
        "                self.co_bin_count[cls]   += binm.T @ binm\n",
        "                self.co_wt_sum[cls]      += vecs.T @ vecs\n",
        "                self.co_wt_sum_both[cls] += (vecs * binm).T @ (vecs * binm)\n",
        "                self.pixel_counter[cls]  += n\n",
        "\n",
        "                # global\n",
        "                self.co_bin_all     += binm.T @ binm\n",
        "                self.co_wt_all      += vecs.T @ vecs\n",
        "                self.co_wt_both_all += (vecs * binm).T @ (vecs * binm)\n",
        "                self.pixel_all      += n\n",
        "\n",
        "                # LINe tensors\n",
        "                self.contrib[cls] += (vecs.sum(0) * w[cls]).abs()\n",
        "                self.avg_act[cls] += (vecs > 0).sum(0)\n",
        "\n",
        "        # ------------------ normalisation -------------------------------\n",
        "        eps = 1e-9\n",
        "        pix_cls = self.pixel_counter.clamp_min(eps).view(self.K, 1)\n",
        "        self.contrib /= pix_cls\n",
        "        self.avg_act /= pix_cls\n",
        "\n",
        "        pc = self.pixel_counter.view(self.K, 1, 1) + eps\n",
        "        self.freq_bin     = self.co_bin_count   / pc\n",
        "        self.mean_wt      = self.co_wt_sum      / pc\n",
        "        self.mean_wt_both = self.co_wt_sum_both / (self.co_bin_count + eps)\n",
        "\n",
        "        diag = torch.diagonal(self.co_bin_count, dim1=1, dim2=2)\n",
        "        cnt_any = diag.unsqueeze(2) + diag.unsqueeze(1) - self.co_bin_count\n",
        "        self.mean_wt_any = self.co_wt_sum / (cnt_any + eps)\n",
        "\n",
        "        self.freq_bin_all     = self.co_bin_all     / (self.pixel_all + eps)\n",
        "        self.mean_wt_all      = self.co_wt_all      / (self.pixel_all + eps)\n",
        "        self.mean_wt_both_all = self.co_wt_both_all / (self.co_bin_all + eps)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #  (De)serialisation helpers\n",
        "    # ------------------------------------------------------------------\n",
        "    def state_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return everything needed to recreate the calibrator later.\"\"\"\n",
        "        return {\n",
        "            \"model_weights\":       self.model.state_dict(),\n",
        "            \"contrib\":             self.contrib.cpu(),\n",
        "            \"avg_act\":             self.avg_act.cpu(),\n",
        "            \"freq_bin\":            self.freq_bin.cpu(),\n",
        "            \"mean_wt\":             self.mean_wt.cpu(),\n",
        "            \"mean_wt_both\":        self.mean_wt_both.cpu(),\n",
        "            \"mean_wt_any\":         self.mean_wt_any.cpu(),\n",
        "            \"freq_bin_all\":        self.freq_bin_all.cpu(),\n",
        "            \"mean_wt_all\":         self.mean_wt_all.cpu(),\n",
        "            \"mean_wt_both_all\":    self.mean_wt_both_all.cpu(),\n",
        "        }\n",
        "\n",
        "    def save(self, path: str) -> None:\n",
        "        torch.save(self.state_dict(), path)\n",
        "        print(f\"[Calibrator] saved → {path}\")\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    @classmethod\n",
        "    def load(\n",
        "        cls,\n",
        "        path: str,\n",
        "        mmseg_cfg: str,\n",
        "        mmseg_ckpt: str,\n",
        "        device: str | None = None,\n",
        "    ) -> \"Calibrator\":\n",
        "        \"\"\"Recreate a Calibrator from disk plus a fresh MMSeg model.\"\"\"\n",
        "        from mmseg.apis import init_segmentor\n",
        "\n",
        "        raw_model = init_segmentor(mmseg_cfg, mmseg_ckpt,\n",
        "                                   device=device or 'cpu')\n",
        "        obj  = cls(raw_model, device)\n",
        "        data = torch.load(path, map_location=obj.dev)\n",
        "\n",
        "        for k, v in data.items():\n",
        "            if k == \"model_weights\":\n",
        "                continue\n",
        "            setattr(obj, k, v.to(obj.dev))\n",
        "\n",
        "        raw_model.load_state_dict(data[\"model_weights\"], strict=False)\n",
        "        print(f\"[Calibrator] loaded ← {path}\")\n",
        "        return obj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "099WE6lHZNPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0412d75-4510-4183-b048-54c9808d5496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import copy\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from mmseg.apis import inference_segmentor\n",
        "from typing import Tuple\n",
        "from sklearn.metrics import (roc_auc_score, roc_curve,\n",
        "                             average_precision_score,\n",
        "                             precision_recall_curve)\n",
        "# För TorchMetrics-baserade metoder\n",
        "from torchmetrics.classification import BinaryAUROC, BinaryAveragePrecision, BinaryROC\n",
        "import os\n",
        "from typing import List\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "#########################################################\n",
        "#  Inference wrapper that applies LINe-style masking    #\n",
        "#  + four pairwise-co-activation variants               #\n",
        "#########################################################\n",
        "class AnomalyDetector:\n",
        "    def __init__(self,\n",
        "                 anomaly_images,\n",
        "                 anomaly_IDs,\n",
        "                 model,\n",
        "                 state_dict):\n",
        "        # ----- unpack model & LINe state -----\n",
        "        self.model                         = model\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.contribution_tensor           = state_dict['contrib'].to(self.device)\n",
        "        self.average_activations_per_class = state_dict['avg_act'].to(self.device)\n",
        "\n",
        "        # ----- thresholds -----\n",
        "        self.activation_clipping     = 1000\n",
        "        self.activation_pruning      = 0\n",
        "        self.weight_pruning          = 0\n",
        "        self.temperature_co          = 1\n",
        "        self.temperature_line        = 1\n",
        "        self.inverse_convert_to_ones = False\n",
        "        self.wt_threshold            = 0\n",
        "        self.wt_u_threshold          = 1\n",
        "        self.co_weighted             = 1\n",
        "        self.max_pool                = 0\n",
        "        self.use_elsewhere_map       = None\n",
        "        self.weight_mode             = \"softmax\"\n",
        "        self.make_feat_binary        = False\n",
        "        self.c_u_ratio               = False\n",
        "\n",
        "        # ----- unpack coact stats -----\n",
        "        self.mean_freq     = state_dict['freq_bin'].to(self.device)      # P[aᵢ>0 ∧ aⱼ>0]     freq_bin is not binary.\n",
        "        self.mean_wt       = state_dict['mean_wt'].to(self.device)      # E[aᵢ·aⱼ]\n",
        "        self.mean_wt_both  = state_dict['mean_wt_both'].to(self.device)  # E[aᵢ·aⱼ | aᵢ>0∧aⱼ>0]\n",
        "        self.mean_wt_any   = state_dict['mean_wt_any'].to(self.device)   # E[aᵢ·aⱼ | aᵢ>0∨aⱼ>0]\n",
        "\n",
        "        # These are not used, yet. They are not class-specific. Any needs an \"all\" as well\n",
        "        self.freq_bin_all     = state_dict['freq_bin'].to(self.device)\n",
        "        self.mean_wt_all       = state_dict['mean_wt'].to(self.device)\n",
        "        self.mean_wt_all       = state_dict['mean_wt_both'].to(self.device)\n",
        "\n",
        "        self.common_freq   = None\n",
        "        self.common_wt     = None\n",
        "        self.common_both   = None\n",
        "        self.common_any    = None\n",
        "        self.uncommon_freq = None\n",
        "        self.uncommon_wt   = None\n",
        "        self.uncommon_both = None\n",
        "        self.uncommon_any  = None\n",
        "\n",
        "        # ----- prepare masks for LINe -----\n",
        "        self.activation_mask     = torch.zeros(19, 512, device=self.device)\n",
        "        self.common_neurons_mask = torch.zeros(19, 512, device=self.device)\n",
        "        self.weight_mask         = torch.zeros(19, 19, 512, device=self.device)\n",
        "\n",
        "        self.model_output      = None\n",
        "        self.line_model_output = [0] * 19\n",
        "        self.model_IDs_output  = None\n",
        "        self.nr_of_activations = None\n",
        "        self.input             = None\n",
        "        self.feature_map       = None\n",
        "\n",
        "        # ----- data & plotting -----\n",
        "        self.anomaly_images = anomaly_images\n",
        "        self.anomaly_IDs    = anomaly_IDs\n",
        "        self.images_to_be_plotted = []\n",
        "        self.plot_pixel_activations = False\n",
        "        self.plot_pair_ratios = False\n",
        "\n",
        "\n",
        "        # ----- fixed conv weight tensor -----\n",
        "        self.weight_tensor = (self.model.decode_head.conv_seg.weight\n",
        "                              .data.squeeze(-1).squeeze(-1))\n",
        "\n",
        "        self.setup_masks()\n",
        "        self.id_dot = (10, 10)\n",
        "        self.ood_dot = (10, 10)\n",
        "\n",
        "        # ----- maps containers -----\n",
        "        self.gt_maps            = []\n",
        "        self.raw_line_maps      = []\n",
        "        self.raw_softmax_maps   = []\n",
        "        self.raw_logitmin_maps  = []\n",
        "        self.raw_logitmax_maps  = []\n",
        "        self.raw_nract_maps     = []\n",
        "        self.raw_euclidian_maps = []\n",
        "        self.raw_coact_bin      = []\n",
        "        self.raw_coact_wt       = []\n",
        "        self.raw_coact_both     = []\n",
        "        self.raw_coact_any      = []\n",
        "\n",
        "        self.co_blur_ksize      = 0\n",
        "        self.clips              = (0,0)\n",
        "        self.co_sigma           = 0\n",
        "        self.baseline_and_line_blur_ksize = (0,0)\n",
        "        self.baseline_and_line_sigma = 0\n",
        "\n",
        "        # ----- register hook on conv_seg -----\n",
        "        self.model.decode_head.conv_seg._forward_hooks.clear()\n",
        "        self.model.decode_head.conv_seg.register_forward_hook(\n",
        "            self.hook\n",
        "        )\n",
        "\n",
        "    def hook(self, module, inp, out):\n",
        "        line_input = inp[0].clone()\n",
        "        self.input       = line_input\n",
        "        self.feature_map = line_input.detach()\n",
        "\n",
        "        # standard conv\n",
        "        pred_logits = F.conv2d(line_input,\n",
        "                                module.weight,\n",
        "                                module.bias,\n",
        "                                stride=module.stride,\n",
        "                                padding=module.padding,\n",
        "                                dilation=module.dilation,\n",
        "                                groups=module.groups)\n",
        "        pred_ids = torch.argmax(pred_logits, dim=1)\n",
        "\n",
        "        # LINe per class (masking + conv). It is possible to remove bias\n",
        "        for cls in range(19):\n",
        "            mask_c = self.activation_mask[cls].view(1,512,1,1)\n",
        "            in2 = line_input.clamp(max=self.activation_clipping) * mask_c\n",
        "            w_m = self.weight_mask[cls].view_as(module.weight)\n",
        "            upd_w = module.weight * w_m\n",
        "            self.line_model_output[cls] = F.conv2d(\n",
        "                in2, upd_w, module.bias,\n",
        "                stride=module.stride,\n",
        "                padding=module.padding,\n",
        "                dilation=module.dilation,\n",
        "                groups=module.groups\n",
        "            )\n",
        "\n",
        "        self.nr_of_activations = (line_input.squeeze(0)>0).sum(dim=0)\n",
        "        self.model_output      = pred_logits\n",
        "        self.model_IDs_output  = pred_ids\n",
        "        return out\n",
        "\n",
        "    def ood_inference(self):\n",
        "        \"\"\"\n",
        "        Run inference, and get raw maps for all methods.\n",
        "        \"\"\"\n",
        "        self.setup_masks()\n",
        "        for i, (img_p, gt_p) in enumerate(zip(self.anomaly_images, self.anomaly_IDs)):\n",
        "            result = inference_segmentor(self.model, img_p)\n",
        "            gt     = cv2.imread(gt_p, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            nr_act    = self._calculate_nr_activation_scores(gt)\n",
        "            line      = self._calculate_line_scores(gt)\n",
        "            softmax   = self._calculate_softmax_scores(gt)\n",
        "            logitmin  = self._calculate_logitmin_scores(gt)\n",
        "            logitmax  = self._calculate_logitmax_scores(gt)\n",
        "            euclidian = self._calculate_euclidean_scores(gt)\n",
        "            coact_any  = self._calculate_coactivation_score(gt, self.common_any, self.uncommon_any)\n",
        "            coact_bin  =  self._calculate_coactivation_score(gt, self.common_freq, self.uncommon_freq)\n",
        "            coact_wt   =  self._calculate_coactivation_score(gt, self.common_wt, self.uncommon_wt)\n",
        "            coact_both = self._calculate_coactivation_score(gt, self.common_both, self.uncommon_both)\n",
        "\n",
        "            # six original maps\n",
        "            self.raw_nract_maps.append(     nr_act)\n",
        "            self.raw_line_maps.append(      line)\n",
        "            self.raw_softmax_maps.append(   softmax)\n",
        "            self.raw_logitmin_maps.append(  logitmin)\n",
        "            self.raw_logitmax_maps.append(  logitmax)\n",
        "            self.raw_euclidian_maps.append( euclidian)\n",
        "\n",
        "            # four coact variants\n",
        "            self.raw_coact_bin.append(      coact_bin)\n",
        "            self.raw_coact_wt.append(       coact_wt)\n",
        "            self.raw_coact_both.append(     coact_both)\n",
        "            self.raw_coact_any.append(      coact_any)\n",
        "\n",
        "            self.gt_maps.append(gt)\n",
        "            if i in self.images_to_be_plotted:\n",
        "                self.plot_maps(result, img_p, gt,\n",
        "                              self.raw_line_maps[-1],\n",
        "                              self.raw_softmax_maps[-1],\n",
        "                              self.raw_logitmin_maps[-1],\n",
        "                              self.raw_logitmax_maps[-1],\n",
        "                              self.raw_euclidian_maps[-1],\n",
        "                              self.raw_nract_maps[-1],\n",
        "                              self.raw_coact_bin[-1],\n",
        "                              self.raw_coact_wt[-1],\n",
        "                              self.raw_coact_both[-1],\n",
        "                              self.raw_coact_any[-1])\n",
        "\n",
        "    def get_ood_score_lists(self):\n",
        "        # flatten GT and filter ignore\n",
        "        all_gt = np.concatenate([m.ravel() for m in self.gt_maps])\n",
        "        valid  = (all_gt != 255)\n",
        "\n",
        "        # collect all ten methods\n",
        "        scores = {\n",
        "            \"ood_scores_line\":          np.concatenate([m.ravel() for m in self.raw_line_maps])[valid],\n",
        "            \"ood_scores_softmax\":       np.concatenate([m.ravel() for m in self.raw_softmax_maps])[valid],\n",
        "            \"ood_scores_logitmin\":      np.concatenate([m.ravel() for m in self.raw_logitmin_maps])[valid],\n",
        "            \"ood_scores_logitmax\":      np.concatenate([m.ravel() for m in self.raw_logitmax_maps])[valid],\n",
        "            \"ood_scores_nr_activation\": np.concatenate([m.ravel() for m in self.raw_nract_maps])[valid],\n",
        "            \"ood_scores_euclidian\":     np.concatenate([m.ravel() for m in self.raw_euclidian_maps])[valid],\n",
        "            \"ood_scores_coact_bin\":     np.concatenate([m.ravel() for m in self.raw_coact_bin])[valid],\n",
        "            \"ood_scores_coact_wt\":      np.concatenate([m.ravel() for m in self.raw_coact_wt])[valid],\n",
        "            \"ood_scores_coact_both\":    np.concatenate([m.ravel() for m in self.raw_coact_both])[valid],\n",
        "            \"ood_scores_coact_any\":     np.concatenate([m.ravel() for m in self.raw_coact_any])[valid],\n",
        "        }\n",
        "        return all_gt[valid], scores\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Score-map calculators\n",
        "    # --------------------------------------------------------------\n",
        "    def _calculate_line_scores(self, labels):\n",
        "        ood = np.zeros((256, 512))\n",
        "        for ID in range(19):\n",
        "            m = (self.model_IDs_output == ID).squeeze(0).cpu().numpy()\n",
        "            logits = self.line_model_output[ID].squeeze(0)\n",
        "            ood[m] = -(torch.logsumexp(logits / self.temperature_line, dim=0) * self.temperature_line).cpu().numpy()[m]\n",
        "\n",
        "        self.use_elsewhere_map = ood #This is done to combine methods first, and do blurring as well, but after.\n",
        "        ood = cv2.resize(ood, labels.shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        return cv2.GaussianBlur(ood, self.baseline_and_line_blur_ksize, self.baseline_and_line_sigma) if self.baseline_and_line_blur_ksize != (0, 0) else ood\n",
        "\n",
        "    def _calculate_logitmax_scores(self, labels):\n",
        "        ood = -torch.max(self.model_output, dim=1).values.squeeze(0).cpu().numpy()\n",
        "        ood = cv2.resize(ood, labels.shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        return cv2.GaussianBlur(ood, self.baseline_and_line_blur_ksize, self.baseline_and_line_sigma) if self.baseline_and_line_blur_ksize != (0, 0) else ood\n",
        "\n",
        "    def _calculate_softmax_scores(self, labels):\n",
        "        sm  = F.softmax(self.model_output, dim=1)\n",
        "        ood = -torch.max(sm, dim=1).values.squeeze(0).cpu().numpy()\n",
        "        ood = cv2.resize(ood, labels.shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        return cv2.GaussianBlur(ood, self.baseline_and_line_blur_ksize, self.baseline_and_line_sigma) if self.baseline_and_line_blur_ksize != (0, 0) else ood\n",
        "\n",
        "    def _calculate_logitmin_scores(self, labels):\n",
        "        ood = -torch.min(self.model_output, dim=1).values.squeeze(0).cpu().numpy()\n",
        "        ood = cv2.resize(ood, labels.shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        return cv2.GaussianBlur(ood, self.baseline_and_line_blur_ksize, self.baseline_and_line_sigma) if self.baseline_and_line_blur_ksize != (0, 0) else ood\n",
        "\n",
        "    def _calculate_nr_activation_scores(self, labels):\n",
        "        ood = np.zeros((256, 512))\n",
        "        for ID in range(19):\n",
        "            act_imp = ((self.input > 0) *\n",
        "                       self.common_neurons_mask[ID].view(1, 512, 1, 1))\n",
        "            m = (self.model_IDs_output == ID).squeeze(0).cpu().numpy()\n",
        "            cur = act_imp.sum(dim=1).squeeze(0).cpu().numpy()\n",
        "            ood[m] = cur[m]\n",
        "        #self.use_elsewhere_map = ood\n",
        "        ood = cv2.resize(ood, labels.shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        return cv2.GaussianBlur(ood, self.baseline_and_line_blur_ksize, self.baseline_and_line_sigma) if self.baseline_and_line_blur_ksize != (0, 0) else ood\n",
        "\n",
        "    def _calculate_euclidean_scores(self, labels, eps=1e-9):\n",
        "        _, _, Hf, Wf = self.input.shape\n",
        "        H_img, W_img = labels.shape\n",
        "        dev          = self.input.device\n",
        "\n",
        "        avg = (self.contribution_tensor /\n",
        "                (self.weight_tensor.abs() + eps)).to(dev)\n",
        "\n",
        "        feat  = self.input.squeeze(0).permute(1,2,0).reshape(-1, 512)\n",
        "        cls   = self.model_IDs_output.squeeze(0).reshape(-1)\n",
        "        dist  = (feat - avg[cls]).pow(2).sum(1).sqrt()\n",
        "\n",
        "        ood = dist.view(Hf, Wf).cpu().numpy()\n",
        "\n",
        "        # If using lowest distance instead of prediction:\n",
        "\n",
        "        # _, _, Hf, Wf = self.input.shape\n",
        "        # H_img, W_img = labels.shape\n",
        "        # dev          = self.input.device\n",
        "\n",
        "        # # avg: [K, C], K = number of classes (e.g. 19), C = feature dim (512)\n",
        "        # avg = (self.contribution_tensor / (self.weight_tensor.abs() + eps)).to(dev)\n",
        "\n",
        "        # # 1) flatten feature map → [N, C]\n",
        "        # feat = (\n",
        "        #     self.input\n",
        "        #         .squeeze(0)                    # [C, Hf, Wf]\n",
        "        #         .permute(1, 2, 0)              # [Hf, Wf, C]\n",
        "        #         .reshape(-1, 512)              # [N, C]\n",
        "        #         .to(dev)\n",
        "        # )\n",
        "\n",
        "        # # 2) compute squared distances to every class-mean → [N, K]\n",
        "        # #    feat.unsqueeze(1): [N, 1, C], avg.unsqueeze(0): [1, K, C]\n",
        "        # dists_sq = (feat.unsqueeze(1) - avg.unsqueeze(0)).pow(2).sum(dim=2)  # [N, K]\n",
        "\n",
        "        # # 3) take sqrt of the minimum over classes → [N]\n",
        "        # min_dist = dists_sq.min(dim=1).values.sqrt()  # [N]\n",
        "\n",
        "        # # 4) reshape back to [Hf, Wf], resize, and blur\n",
        "        # ood = min_dist.view(Hf, Wf).cpu().numpy()\n",
        "\n",
        "        ood = cv2.resize(ood, labels.shape[::-1], interpolation=cv2.INTER_NEAREST)\n",
        "        return cv2.GaussianBlur(ood, self.baseline_and_line_blur_ksize, self.baseline_and_line_sigma) if self.baseline_and_line_blur_ksize != (0, 0) else ood\n",
        "\n",
        "    def setup_masks(self):\n",
        "        # initialize accumulators for per-class weighting\n",
        "        weighted_freq  = torch.zeros_like(self.mean_freq)\n",
        "        weighted_wt    = torch.zeros_like(self.mean_wt)\n",
        "        weighted_both  = torch.zeros_like(self.mean_wt_both)\n",
        "        weighted_any   = torch.zeros_like(self.mean_wt_any)\n",
        "\n",
        "\n",
        "        # Weight matrix used for experimenting with weights for the Co-act matrix. Can be ignored\n",
        "        weight_co_tensor = torch.zeros_like(self.mean_freq)\n",
        "        for ID in range(19):\n",
        "            w_raw = self.weight_tensor[ID].view(-1,1)\n",
        "            w_pos = torch.relu(w_raw)          # positives\n",
        "            w_neg = torch.relu(-w_raw)         # negatives as positives\n",
        "            w_abs = w_raw.abs()                # magnitude\n",
        "            w_raw = self.weight_tensor[ID].view(-1,1)\n",
        "            pos   = (w_raw > 0).float()   # 1 where wᵢ>0\n",
        "            neg   = (w_raw < 0).float()   # 1 where wᵢ<0\n",
        "            # 1) only positive–positive pairs\n",
        "            mat_pospos = pos @ pos.T        # ones only where wᵢ>0 AND wⱼ>0\n",
        "\n",
        "            # 2) only negative–negative pairs\n",
        "            mat_negneg = neg @ neg.T        # ones only where wᵢ<0 AND wⱼ<0\n",
        "\n",
        "            # 3) only negative–positive (i negative, j positive)\n",
        "            mat_negpos = neg @ pos.T        # ones only where wᵢ<0 AND wⱼ>0\n",
        "\n",
        "            # (optionally the flip: positive–negative)\n",
        "            mat_posneg = pos @ neg.T        # ones only where wᵢ>0 AND wⱼ<0\n",
        "\n",
        "\n",
        "            if   self.co_weighted ==  0:\n",
        "              weight_matrix = torch.ones_like(self.mean_freq[ID])\n",
        "            elif self.co_weighted ==  1:\n",
        "                weight_matrix = w_pos @ w_pos.T\n",
        "            elif self.co_weighted ==  2:\n",
        "                weight_matrix = w_pos + w_pos.T\n",
        "            elif self.co_weighted ==  3:\n",
        "                weight_matrix = w_neg @ w_neg.T\n",
        "            elif self.co_weighted ==  4:\n",
        "                weight_matrix = w_neg + w_neg.T\n",
        "            elif self.co_weighted ==  5:\n",
        "                weight_matrix = w_raw @ w_raw.T\n",
        "            elif self.co_weighted ==  6:\n",
        "                weight_matrix = -(w_raw + w_raw.T)\n",
        "            elif self.co_weighted ==  7:\n",
        "                weight_matrix = w_abs @ w_abs.T\n",
        "            elif self.co_weighted ==  8:\n",
        "                weight_matrix = w_abs + w_abs.T\n",
        "            elif self.co_weighted ==  9:\n",
        "                weight_matrix = w_pos + w_neg.T\n",
        "            elif self.co_weighted == 10:\n",
        "                weight_matrix = w_neg + w_pos.T\n",
        "            elif self.co_weighted == 11:\n",
        "                weight_matrix = mat_pospos\n",
        "            elif self.co_weighted == 12:\n",
        "                weight_matrix = mat_negneg\n",
        "            elif self.co_weighted == 13:\n",
        "                weight_matrix = mat_negpos\n",
        "            elif self.co_weighted == 14:\n",
        "                weight_matrix = mat_posneg\n",
        "            elif   self.co_weighted == 15:\n",
        "                # broadcast: [-w_neg] is [C,1], w_pos.T is [1,C] → result [C,C]\n",
        "                weight_matrix = -(-w_neg + w_pos.T)\n",
        "            elif self.co_weighted == 16:\n",
        "                # sum of absolute weights, but keep the original sign of (w_i + w_j)\n",
        "                w_sum = w_raw + w_raw.T                # [C,C]\n",
        "                m     = w_abs + w_abs.T                # [C,C]\n",
        "                weight_matrix = -m * torch.sign(w_sum) # [C,C]\n",
        "            elif self.co_weighted == 17:\n",
        "                weight_matrix = w_neg @ w_pos.T\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown co_weighted={self.co_weighted}\")\n",
        "\n",
        "            weight_co_tensor[ID] = weight_matrix\n",
        "            weighted_freq[ID] = self.mean_freq[ID]\n",
        "            weighted_wt[ID] = self.mean_wt[ID]\n",
        "            weighted_both[ID] = self.mean_wt_both[ID]\n",
        "            weighted_any[ID] = self.mean_wt_any[ID]\n",
        "\n",
        "        # Build binary masks\n",
        "        self.common_freq_mask = (weighted_freq  >= torch.quantile(weighted_freq.view(19, -1),\n",
        "                                                                  self.wt_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        self.common_wt_mask = (weighted_wt  >= torch.quantile(weighted_wt.view(19, -1),\n",
        "                                                              self.wt_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        self.common_both_mask = (weighted_both >= torch.quantile(weighted_both.view(19, -1),\n",
        "                                                                self.wt_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        self.common_any_mask = (weighted_any >= torch.quantile(weighted_any.view(19, -1),\n",
        "                                                              self.wt_threshold, dim=1).view(19, 1, 1)).float()\n",
        "\n",
        "        self.uncommon_freq_mask = (weighted_freq <= torch.quantile(weighted_freq.view(19, -1),\n",
        "                                                                  self.wt_u_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        self.uncommon_wt_mask = (weighted_wt <= torch.quantile(weighted_wt.view(19, -1),\n",
        "                                                              self.wt_u_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        self.uncommon_both_mask = (weighted_both <= torch.quantile(weighted_both.view(19, -1),\n",
        "                                                                  self.wt_u_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        self.uncommon_any_mask = (weighted_any <= torch.quantile(weighted_any.view(19, -1),\n",
        "                                                                self.wt_u_threshold, dim=1).view(19, 1, 1)).float()\n",
        "        # Create inverses\n",
        "        eps = 1e-12\n",
        "        self.common_freq = self.common_freq_mask / (weighted_freq + eps) * weight_co_tensor\n",
        "        self.common_wt = self.common_wt_mask / (weighted_wt + eps) * weight_co_tensor\n",
        "        self.common_both = self.common_both_mask / (weighted_both + eps) * weight_co_tensor\n",
        "        self.common_any = self.common_any_mask / (weighted_any + eps) * weight_co_tensor\n",
        "\n",
        "        self.uncommon_freq = self.uncommon_freq_mask / (weighted_freq + eps) * weight_co_tensor\n",
        "        self.uncommon_wt = self.uncommon_wt_mask / (weighted_wt + eps) * weight_co_tensor\n",
        "        self.uncommon_both = self.uncommon_both_mask / (weighted_both + eps) * weight_co_tensor\n",
        "        self.uncommon_any = self.uncommon_any_mask / (weighted_any + eps) * weight_co_tensor\n",
        "\n",
        "        # Create LINe masks\n",
        "        for ID in range(19):\n",
        "            thr = torch.quantile(self.contribution_tensor[ID], self.activation_pruning)\n",
        "            self.activation_mask[ID] = (self.contribution_tensor[ID] >= thr).float()\n",
        "            self.weight_mask[ID] = self.weight_tensor * self.contribution_tensor[ID]\n",
        "            thr_w = torch.quantile(self.weight_mask[ID], self.weight_pruning)\n",
        "            self.weight_mask[ID] = (self.weight_mask[ID] >= thr_w).float()\n",
        "            thr_c = torch.quantile(self.average_activations_per_class[ID], 0)                        # To include common neurons instead of combinations, can be ignored\n",
        "            self.common_neurons_mask[ID] = (self.average_activations_per_class[ID] > thr_c).float()  # To include common neurons instead of combinations, can be ignored\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Visualisation\n",
        "    # --------------------------------------------------------------\n",
        "    def plot_maps(self, result, img_path, gt, line_map, softmax_map, logitmin_map, logitmax_map, euclidian_map, nr_act_map, coact_map1, coact_map2, coact_map3, coact_map4):\n",
        "        seg_map = result[0]\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        H_img, W_img = gt.shape\n",
        "\n",
        "        dots = [self.ood_dot, self.id_dot]\n",
        "\n",
        "        # To plot the penultimate activations. Modify the function however you want to plot it (for example uncommon to the left, or low contributing to the left)\n",
        "        if self.plot_pixel_activations:\n",
        "            self.plot_pixel_activation((self.ood_dot[0], self.ood_dot[1]), kind=\"OOD\",    img_shape=(H_img, W_img))\n",
        "            self.plot_pixel_activation((self.id_dot[0], self.ood_dot[1]),  kind=\"Normal\", img_shape=(H_img, W_img))\n",
        "        if self.plot_pair_ratios:\n",
        "            self.plot_pair_ratio(self.ood_dot, (H_img, W_img), nbins=50)\n",
        "            self.plot_pair_ratio(self.id_dot,  (H_img, W_img), nbins=50)\n",
        "\n",
        "\n",
        "        # Show all heatmaps\n",
        "        print(\"nr activations:\")\n",
        "        self.plotter(img, seg_map, nr_act_map, gt, dots)\n",
        "        print(\"line:\")\n",
        "        self.plotter(img, seg_map, line_map, gt, dots)\n",
        "        print(\"softmax:\")\n",
        "        self.plotter(img, seg_map, softmax_map, gt, dots)\n",
        "        print(\"logitmin:\")\n",
        "        self.plotter(img, seg_map, logitmin_map, gt, dots)\n",
        "        print(\"logitmax:\")\n",
        "        self.plotter(img, seg_map, logitmax_map, gt, dots)\n",
        "        print(\"euclidian:\")\n",
        "        self.plotter(img, seg_map, euclidian_map, gt, dots)\n",
        "        print(\"coact_bin:\")\n",
        "        self.plotter(img, seg_map, coact_map1, gt, dots)\n",
        "        print(\"coact_wt:\")\n",
        "        self.plotter(img, seg_map, coact_map2, gt, dots)\n",
        "        print(\"coact_both:\")\n",
        "        self.plotter(img, seg_map, coact_map3, gt, dots)\n",
        "        print(\"coact_any:\")\n",
        "        self.plotter(img, seg_map, coact_map4, gt, dots)\n",
        "\n",
        "    def plotter(self,\n",
        "                            img: np.ndarray,\n",
        "                            seg_map: np.ndarray,\n",
        "                            ood_map: np.ndarray,\n",
        "                            gt: np.ndarray,\n",
        "                            dots: List[Tuple[int,int,str]] = None):\n",
        "        \"\"\"\n",
        "        Creates a 2×2 figure:\n",
        "          [ (1,1) RGB + seg overlay       | (1,2) raw heatmap             ]\n",
        "          [ (2,1) masked heatmap (ignore)$ | (2,2) ground‐truth (ignore)$  ]\n",
        "\n",
        "        Arguments:\n",
        "            img     : H×W×3 RGB image (uint8)\n",
        "            seg_map : H×W segmentation map in [0..18] that you overlay with alpha\n",
        "            ood_map : H×W float array (raw OOD‐scores)\n",
        "            gt      : H×W integer ground truth with {0,1,…,18, 255=ignore}\n",
        "            dots    : optional list of (y, x, color) to scatter\n",
        "        \"\"\"\n",
        "\n",
        "        ignore_label = 255\n",
        "\n",
        "        # Precompute vmin/vmax for the raw heatmap\n",
        "        vmin, vmax = float(ood_map.min()), float(ood_map.max())\n",
        "\n",
        "        # Build the masked‐OOD map (ignore pixels → masked → black)\n",
        "        masked_ood = np.ma.masked_where(gt == ignore_label, ood_map)\n",
        "        cmap_masked = plt.cm.jet.copy()\n",
        "        cmap_masked.set_bad(color=\"black\")\n",
        "\n",
        "        gt_only = gt.copy()\n",
        "\n",
        "        # Mask wherever gt == 255:\n",
        "        gt_masked = np.ma.masked_where(gt_only == ignore_label, gt_only)\n",
        "\n",
        "        cmap_gt = ListedColormap([\"darkgreen\", \"crimson\"])  # 0→green, 1→red\n",
        "        cmap_gt.set_bad(color=\"black\")                       # 255→black\n",
        "\n",
        "        plt.figure(figsize=(16, 12))\n",
        "\n",
        "        # ---------------------------------------\n",
        "        #  Panel (1,1): RGB + segmentation overlay\n",
        "        # ---------------------------------------\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(seg_map, cmap=\"jet\", alpha=0.4)\n",
        "        if dots is not None:\n",
        "            for (y, x, c) in dots:\n",
        "                plt.scatter(x, y, s=10, c=c, marker=\"o\",\n",
        "                            edgecolors=\"white\", linewidths=1, zorder=5)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"RGB + Segmentation Overlay\")\n",
        "\n",
        "        # -------------------------------\n",
        "        #  Panel (1,2): RAW OOD heatmap\n",
        "        # -------------------------------\n",
        "        plt.subplot(2, 2, 2)\n",
        "        im_raw = plt.imshow(ood_map, cmap=\"jet\", vmin=vmin, vmax=vmax)\n",
        "        if dots is not None:\n",
        "            for (y, x, c) in dots:\n",
        "                plt.scatter(x, y, s=10, c=c, marker=\"o\",\n",
        "                            edgecolors=\"white\", linewidths=1, zorder=5)\n",
        "        plt.colorbar(im_raw, fraction=0.046, pad=0.04, format=\"%.2f\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Raw OOD Heatmap\")\n",
        "\n",
        "        # -----------------------------------------------\n",
        "        #  Panel (2,1): OOD heatmap with ignore as black\n",
        "        # -----------------------------------------------\n",
        "        plt.subplot(2, 2, 3)\n",
        "        im_masked = plt.imshow(masked_ood, cmap=cmap_masked, vmin=vmin, vmax=vmax)\n",
        "        if dots is not None:\n",
        "            for (y, x, c) in dots:\n",
        "                plt.scatter(x, y, s=20, c=c, marker=\"o\",\n",
        "                            edgecolors=\"white\", linewidths=1.5, zorder=5)\n",
        "        plt.colorbar(im_masked, fraction=0.046, pad=0.04, format=\"%.2f\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"OOD Heatmap (ignore=black)\")\n",
        "\n",
        "        # ------------------------------------\n",
        "        #  Panel (2,2): Ground Truth only\n",
        "        # ------------------------------------\n",
        "        plt.subplot(2, 2, 4)\n",
        "        im_gt = plt.imshow(gt_masked, cmap=cmap_gt, vmin=0, vmax=1)\n",
        "        cbar2 = plt.colorbar(im_gt, ticks=[0, 1], fraction=0.046, pad=0.04)\n",
        "        cbar2.set_label(\"GT label (0=ID, 1=OOD)\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Ground Truth (ignore=black)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_pixel_activation(self, pixel: Tuple[int,int], kind: str,\n",
        "                              img_shape: Tuple[int,int]):\n",
        "        y, x = pixel; H_img, W_img = img_shape\n",
        "        _, _, Hf, Wf = self.input.shape\n",
        "        yf, xf = int(y*Hf/H_img), int(x*Wf/W_img)\n",
        "        act = self.input[0, :, yf, xf].cpu().numpy()\n",
        "        weights = (self.model.decode_head.conv_seg.weight\n",
        "                   .data.squeeze(-1).squeeze(-1).cpu().numpy())\n",
        "        pred_id = int(self.model_IDs_output[0, yf, xf])\n",
        "        contrib = self.average_activations_per_class[pred_id].cpu().numpy()\n",
        "        order   = np.argsort(contrib)\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(act[order]*weights[pred_id][order], marker='o', ms=2)\n",
        "        plt.title(f'{kind} pixel ({y},{x}) pred={pred_id}')\n",
        "        plt.xlabel('channel rank'); plt.ylabel('activation'); plt.tight_layout(); plt.show()\n",
        "\n",
        "    def plot_pair_ratio(self,\n",
        "                        pixel: Tuple[int, int],\n",
        "                        img_shape: Tuple[int, int],\n",
        "                        cls:   int = None,\n",
        "                        nbins: int = None):\n",
        "        y_img, x_img       = pixel[:2]\n",
        "        H_img, W_img       = img_shape\n",
        "        _, _, Hf, Wf       = self.feature_map.shape\n",
        "        yf = int(y_img * Hf / H_img)\n",
        "        xf = int(x_img * Wf / W_img)\n",
        "\n",
        "        # 2) extract feature-vector at that location\n",
        "        v = self.feature_map[0, :, yf, xf].detach().cpu().numpy()   # [512]\n",
        "        if cls is None:\n",
        "            cls = int(self.model_IDs_output[0, yf, xf])\n",
        "\n",
        "        # 3) raw ratio for every (i,j) pair\n",
        "        raw     = np.outer(v, v).ravel()                            # [512²]\n",
        "        mean_ij = self.mean_wt_any[cls].cpu().numpy().ravel()       # [512²]\n",
        "        ratio   = raw / (mean_ij + 1e-9)\n",
        "\n",
        "        # 4) order by rarity\n",
        "        rarity = self.uncommon_any[cls].cpu().numpy().ravel()\n",
        "        order  = np.argsort(rarity)[::-1]\n",
        "        ratio_sorted = ratio[order]\n",
        "\n",
        "        # 5) raw vs. binned plot\n",
        "        if nbins is None or nbins <= 1:\n",
        "            x = np.arange(ratio_sorted.size)\n",
        "            y = ratio_sorted\n",
        "            xlabel = \"Channel-pair rank (rarest → most common)  [raw 262 144 points]\"\n",
        "        else:\n",
        "            groups = np.array_split(ratio_sorted, nbins)\n",
        "            y = np.array([g.mean() for g in groups])\n",
        "\n",
        "            # map each bin back onto the original rank axis\n",
        "            edges = np.linspace(0, ratio_sorted.size, nbins + 1)\n",
        "            x     = 0.5 * (edges[1:] + edges[:-1])      # centre rank of each bin\n",
        "\n",
        "            xlabel = \"Channel-pair rank (rarest → most common)\"\n",
        "\n",
        "        # 6) draw\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.plot(x, y, lw=1)\n",
        "        plt.axhline(1.0, color=\"k\", ls=\"--\", lw=1)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(r\"$(v_i v_j)\\,/\\,E[a_i a_j]$\")\n",
        "        plt.title(f\"Pixel ({y_img},{x_img}), class {cls}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Metric computation\n",
        "    # --------------------------------------------------------------\n",
        "    def calculate_metrics(\n",
        "            self,\n",
        "            gt_ids: np.ndarray,\n",
        "            scores: np.ndarray,\n",
        "            device: str = \"cuda\",\n",
        "            batch_size: int = 100_000,\n",
        "            plot_hist: bool = False,              # ← NEW SWITCH\n",
        "            nbins: int = 100,                     # ← bins for the histogram\n",
        "            title: str = \"OOD score distribution\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Calculates AUROC, FPR@95 % TPR and AP with TorchMetrics\n",
        "        *and* (optionally) plots a histogram of the score distribution, as well\n",
        "        as an average precision curve.\n",
        "        \"\"\"\n",
        "\n",
        "        # ────────────────────────────────────────────────────────────────\n",
        "        # 1. metrics\n",
        "        g_all = torch.from_numpy(gt_ids).to(device)\n",
        "        s_all = torch.from_numpy(scores).float().to(device)\n",
        "\n",
        "        s_all = s_all - s_all.min()\n",
        "        s_all = s_all / s_all.max()\n",
        "\n",
        "        auroc = BinaryAUROC().to(device)\n",
        "        ap    = BinaryAveragePrecision().to(device)\n",
        "        roc   = BinaryROC().to(device)\n",
        "\n",
        "        for i in range(0, g_all.numel(), batch_size):\n",
        "            auroc.update(s_all[i:i+batch_size], g_all[i:i+batch_size].int())\n",
        "            ap.update(   s_all[i:i+batch_size], g_all[i:i+batch_size].int())\n",
        "            roc.update(  s_all[i:i+batch_size], g_all[i:i+batch_size].int())\n",
        "\n",
        "        auc  = auroc.compute().item()\n",
        "        ap_v = ap.compute().item()\n",
        "        fpr, tpr, _ = roc.compute()\n",
        "        fpr95 = fpr[tpr >= 0.95][0].item()\n",
        "\n",
        "        print(f\"AUROC={auc:.4f}  FPR95={fpr95:.4f}  AP={ap_v:.4f}\")\n",
        "\n",
        "        # ────────────────────────────────────────────────────────────────\n",
        "        # histogram\n",
        "        if plot_hist:\n",
        "            id_scores  = scores[gt_ids == 0]\n",
        "            ood_scores = scores[gt_ids == 1]\n",
        "\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.hist(id_scores,  bins=nbins, alpha=0.6, density=True,\n",
        "                    label=f'ID  (n={len(id_scores):,})')\n",
        "            plt.hist(ood_scores, bins=nbins, alpha=0.6, density=True,\n",
        "                    label=f'OOD (n={len(ood_scores):,})')\n",
        "            plt.xlabel(\"OOD score\");  plt.ylabel(\"Density\")\n",
        "            plt.title(title)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        plot_pr = False\n",
        "        if plot_pr:\n",
        "\n",
        "            # gt_ids is a NumPy array of 0 (ID) / 1 (OOD), scores is the OOD‐score\n",
        "            precision, recall, _ = precision_recall_curve(gt_ids, scores)\n",
        "            ap_scalar = average_precision_score(gt_ids, scores)\n",
        "\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.plot(\n",
        "                recall,\n",
        "                precision,\n",
        "                lw=1.5,\n",
        "                label=f\"PR curve (AP={100*ap_scalar:.2f}%)\"\n",
        "            )\n",
        "            plt.xlabel(\"Recall\")\n",
        "            plt.ylabel(\"Precision\")\n",
        "            plt.title(\"Precision–Recall Curve\")\n",
        "            plt.legend(loc=\"lower left\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return auc, fpr95, ap_v\n",
        "\n",
        "    def _calculate_coactivation_score(\n",
        "        self,\n",
        "        labels: np.ndarray,\n",
        "        common_inverse: torch.Tensor,\n",
        "        uncommon_inverse: torch.Tensor,\n",
        "        chunk_size: int = 4096,\n",
        "        abs_logits: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Returns an OOD-heat-map where each class’s co-activation ratio\n",
        "        is weighted by either its softmax probability\n",
        "        or absolute logit, then summed over classes.\n",
        "        \"\"\"\n",
        "        if self.feature_map is None:\n",
        "            raise RuntimeError(\"feature_map not cached by hook!\")\n",
        "\n",
        "        # dimensions & device\n",
        "        _, _, Hf, Wf = self.feature_map.shape\n",
        "        H_img, W_img = labels.shape\n",
        "        N            = Hf * Wf\n",
        "        dev          = self.device\n",
        "        eps          = 1e-9\n",
        "\n",
        "        C = self.feature_map.size(1)           # e.g. 512\n",
        "        K = self.model_output.size(1)          # number of classes, e.g. 19\n",
        "\n",
        "        # Flatten feature map → [N,C], logits → [N,K], all on GPU\n",
        "        feat = (\n",
        "            self.feature_map\n",
        "                .squeeze(0)                    # [C,Hf,Wf]\n",
        "                .permute(1,2,0)                # [Hf,Wf,C]\n",
        "                .reshape(-1, C)                # [N,C]\n",
        "                .to(dev, non_blocking=True)\n",
        "        )\n",
        "        if self.make_feat_binary == True:\n",
        "            feat = (feat > 0).float()\n",
        "\n",
        "        logits_flat = (\n",
        "            self.model_output\n",
        "                .squeeze(0)                    # [K,Hf,Wf]\n",
        "                .permute(1,2,0)                # [Hf,Wf,K]\n",
        "                .reshape(-1, K)                # [N,K]\n",
        "                .to(dev, non_blocking=True)\n",
        "        )\n",
        "\n",
        "        # Choose weights: softmax probs or abs(logit)\n",
        "        if self.weight_mode == 'softmax':\n",
        "            w_all = torch.softmax(logits_flat / self.temperature_co, dim=1)\n",
        "\n",
        "        elif self.weight_mode == 'logits':\n",
        "            # raw logits\n",
        "            w_all = logits_flat\n",
        "\n",
        "        elif self.weight_mode == 'pred':\n",
        "            # Only predicted class\n",
        "            pred_ids = torch.argmax(logits_flat, dim=1)                # [N]\n",
        "            w_all = torch.zeros_like(logits_flat)\n",
        "            w_all.scatter_(1, pred_ids.unsqueeze(1), 1.0)              # one-hot per row\n",
        "        else:\n",
        "            raise ValueError(f\"unknown weight_mode={self.weight_mode}\")\n",
        "\n",
        "        # 3) Prepare binary masks & normalisers on GPU\n",
        "        if self.inverse_convert_to_ones == True:\n",
        "            M_c = (common_inverse > 0).to(dev).float()       # [K,C,C]\n",
        "            M_u = (uncommon_inverse > 0).to(dev).float()     # [K,C,C]\n",
        "        else:\n",
        "            M_c = common_inverse.to(dev).float()       # [K,C,C]\n",
        "            M_u = uncommon_inverse.to(dev).float()     # [K,C,C]\n",
        "        # n_c = M_c.view(K, -1).sum(1).clamp_min(eps)   # [K] # Class normalisation that is WRONG, but can actually improve performance(cheating)\n",
        "        # n_u = M_u.view(K, -1).sum(1).clamp_min(eps)   # [K] # Class normalisation that is WRONG, but can actually improve performance(cheating)\n",
        "\n",
        "        # flatten masks for GEMM\n",
        "        M_c2 = M_c.view(K*C, C)  # [K*C, C]\n",
        "        M_u2 = M_u.view(K*C, C)\n",
        "\n",
        "        ood_out = torch.empty(N, device=dev)\n",
        "\n",
        "        # Chunked GPU loop\n",
        "        for s in range(0, N, chunk_size):\n",
        "            e = min(N, s + chunk_size)\n",
        "            v = feat[s:e]                           # [B, C]\n",
        "            B = v.size(0)\n",
        "\n",
        "            # project v onto each class-inverse-matrix\n",
        "            pc = (v @ M_c2.T).view(B, K, C)         # [B, K, C]\n",
        "            pu = (v @ M_u2.T).view(B, K, C)         # [B, K, C]\n",
        "\n",
        "            # co-activation sums\n",
        "            s_c = (v.unsqueeze(1) * pc).sum(dim=2)# / n_c   # [B, K]\n",
        "            s_u = (v.unsqueeze(1) * pu).sum(dim=2)# / n_u   # [B, K]\n",
        "\n",
        "            if self.c_u_ratio == True:\n",
        "                co_ratio = s_u/s_c # [B, K]\n",
        "            else:\n",
        "                co_ratio = s_u     # [B, K]\n",
        "\n",
        "            # weight, sum over classes → [B], clamp\n",
        "            score = (co_ratio * w_all[s:e]).sum(dim=1)                       # [B]\n",
        "            if self.clips != (0, 0):\n",
        "                score = torch.clamp(score, min=self.clips[0], max=self.clips[1]) # [B]\n",
        "            ood_out[s:e] = score\n",
        "\n",
        "        ood_map_gpu = ood_out.view(1, 1, Hf, Wf)          # [1,1,Hf,Wf]\n",
        "\n",
        "        if self.max_pool != 0:\n",
        "            ood_map_gpu = F.max_pool2d(ood_map_gpu, kernel_size=self.max_pool, stride=1, padding=self.max_pool//2)\n",
        "        if self.co_blur_ksize != 0:\n",
        "            k = self.co_blur_ksize\n",
        "            σ = float(self.co_sigma)\n",
        "            if σ <= 0:\n",
        "                σ = 1\n",
        "\n",
        "            coords = torch.arange(k, device=dev) - (k // 2)\n",
        "            g1d = torch.exp(-coords.float().pow(2) / (2 * σ * σ))\n",
        "            g1d /= g1d.sum()\n",
        "            kernel2d = (g1d[:, None] @ g1d[None, :]).view(1, 1, k, k).to(dev)\n",
        "            ood_map_gpu = F.conv2d(ood_map_gpu, kernel2d, padding=k // 2)\n",
        "\n",
        "        ood_map = ood_map_gpu.squeeze().cpu().numpy()    # [Hf,Wf]\n",
        "        # np.log(ood_map, out=ood_map) #if they have to big differences\n",
        "\n",
        "        # Uncomment if you want to do divide by another map and do additional blurring after\n",
        "        # ood_map = -ood_map/self.use_elsewhere_map/self.use_elsewhere_map/self.use_elsewhere_map\n",
        "        # do numpy gaussian blur here\n",
        "\n",
        "        ood_map = cv2.resize(\n",
        "            ood_map,\n",
        "            (W_img, H_img),                             # width, height\n",
        "            interpolation=cv2.INTER_NEAREST\n",
        "        )\n",
        "        return ood_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "gWzNO72LsvDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd046a34-a1b7-4200-e1b4-75e47fe1c05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2975 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth\n",
            "load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth\n",
            "[Calibrator] loaded ← /content/drive/MyDrive/calibrator_state_ground_truth.pt\n"
          ]
        }
      ],
      "source": [
        "# notebook_or_script.py  ──────────────────────────────────────────────\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from mmseg.apis import init_segmentor          # ← plain MMSeg import\n",
        "#from calibrator import Calibrator             # ← the file you showed\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1.  Collect training image / GT paths  (unchanged helper)\n",
        "# --------------------------------------------------------------------\n",
        "def cityscapes_paths(img_root: str, gt_root: str) -> Tuple[List[str], List[str]]:\n",
        "    images, gts = [], []\n",
        "    for city in sorted(os.listdir(img_root)):\n",
        "        d_img, d_gt = os.path.join(img_root, city), os.path.join(gt_root, city)\n",
        "        if not os.path.isdir(d_img):\n",
        "            continue\n",
        "        for fn in sorted(os.listdir(d_img)):\n",
        "            if not fn.endswith('_leftImg8bit.png'):\n",
        "                continue\n",
        "            images.append(os.path.join(d_img, fn))\n",
        "            label_fn = fn.replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
        "            gts.append(os.path.join(d_gt, label_fn))\n",
        "    return images, gts\n",
        "\n",
        "\n",
        "img_root = '/content/drive/MyDrive/leftImg8bit_trainvaltest/leftImg8bit/train'\n",
        "gt_root  = '/content/drive/MyDrive/gtFine_trainvaltest/gtFine/train'\n",
        "images, gts = cityscapes_paths(img_root, gt_root)\n",
        "print(f'Found {len(images)} images')\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2.  Bring up the MMSeg model  (one line)\n",
        "# --------------------------------------------------------------------\n",
        "CFG  = 'configs/deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes.py'\n",
        "CKPT = ('https://download.openmmlab.com/mmsegmentation/v0.5/'\n",
        "        'deeplabv3plus/deeplabv3plus_r101-d8_512x1024_40k_cityscapes/'\n",
        "        'deeplabv3plus_r101-d8_512x1024_40k_cityscapes_20200605_094614-3769eecf.pth')\n",
        "\n",
        "DEV = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "mmseg_model = init_segmentor(CFG, CKPT, device=DEV)   # <-- raw MMSeg model\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3.  Calibrate (or reload previous statistics)\n",
        "# --------------------------------------------------------------------\n",
        "SAVE = '/content/drive/MyDrive/calibrator_state_ground_truth.pt'\n",
        "\n",
        "if os.path.exists(SAVE):\n",
        "    # one-liner restore: builds a *fresh* model internally\n",
        "    calib = Calibrator.load(SAVE, mmseg_cfg=CFG, mmseg_ckpt=CKPT, device=DEV)\n",
        "else:\n",
        "    calib = Calibrator(mmseg_model)    # hand the model in\n",
        "    calib.run(images, gts)             # long pass over the dataset\n",
        "    calib.save(SAVE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYw_BwdSNOHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d16b9c-5c48-4155-9076-3d2674f83341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9501  FPR95=0.3238  AP=0.2127\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9575  FPR95=0.1893  AP=0.2036\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9618  FPR95=0.1607  AP=0.2072\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9559  FPR95=0.1995  AP=0.2028\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9163  FPR95=0.4390  AP=0.1225\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9163  FPR95=0.4390  AP=0.1225\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9163  FPR95=0.4390  AP=0.1225\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9683  FPR95=0.1407  AP=0.3164\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9683  FPR95=0.1407  AP=0.3164\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9683  FPR95=0.1407  AP=0.3164\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 1000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9421  FPR95=0.2579  AP=0.1246\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9656  FPR95=0.1870  AP=0.2750\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9668  FPR95=0.1698  AP=0.2687\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9562  FPR95=0.2107  AP=0.1776\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9452  FPR95=0.2482  AP=0.2002\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9490  FPR95=0.2302  AP=0.2025\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(250000, 10000000) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9486  FPR95=0.3242  AP=0.1044\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9636  FPR95=0.1888  AP=0.1331\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9662  FPR95=0.1698  AP=0.1530\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9527  FPR95=0.2124  AP=0.0693\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9438  FPR95=0.1810  AP=0.0427\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9570  FPR95=0.1530  AP=0.0708\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=False | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9163  FPR95=0.4390  AP=0.1225\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9163  FPR95=0.4390  AP=0.1225\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9163  FPR95=0.4390  AP=0.1225\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=False | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9683  FPR95=0.1407  AP=0.3164\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9683  FPR95=0.1407  AP=0.3164\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9683  FPR95=0.1407  AP=0.3164\n",
            "method | =ood_scores_nr_activationtemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=False\n",
            "AUROC=0.9567  FPR95=0.2035  AP=0.2439\n",
            "method | =ood_scores_coact_bintemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_wttemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n",
            "AUROC=0.9427  FPR95=0.2597  AP=0.1997\n",
            "method | =ood_scores_coact_anytemp_co=0.3 | activation_clipping=100 | activation_pruning=0 | weight_pruning=0 | wt_pair=(0, 1) | co_blur_pair=(23, 5) | clip_val=(0, 0) | temp_line=1 | co_weighted=0 | inv2ones=True | bin_or_not=True | baseline_pair=((3, 3), 0) | max_pool=11 | weight_mode=softmax | c_u_ratio=True\n"
          ]
        }
      ],
      "source": [
        "# ╔════════════════════════════════════════╗\n",
        "# ║  Grid-search for LINe + Co-activation  ║\n",
        "# ╚════════════════════════════════════════╝\n",
        "\n",
        "import itertools\n",
        "import gc\n",
        "\n",
        "start, end = 0, 100     # Max 100\n",
        "matched_images_short      = matched_images[start:end]\n",
        "matched_annotations_short = matched_annotations[start:end]\n",
        "images_to_be_plotted      = []  # show figures only for the first image if 0, can use 3:7 to plot image 4-8 for example. Empty for no figures\n",
        "\n",
        "# Hyper-parameter lists (edit to sweep more values)\n",
        "activation_clippings            = [100]\n",
        "activation_prunings             = [0]\n",
        "weight_prunings                 = [0]\n",
        "temperatures_line               = [1]\n",
        "\n",
        "inverse_convert_to_ones         = [False, True]\n",
        "binary_or_not                   = [False, True]\n",
        "wt_thresholds                   = [0, 0]\n",
        "wt_u_thresholds                 = [1, 0.1]\n",
        "temperatures_co                 = [0.3, 1]\n",
        "\n",
        "baseline_and_line_blur_ksizes   = [(3, 3)]\n",
        "baseline_and_line_sigmas        = [0]\n",
        "\n",
        "co_blur_sizes                   = [23]\n",
        "co_sigmas                       = [5]\n",
        "max_pools                       = [11]\n",
        "\n",
        "clips                           = [(0, 1000000), (250000, 10000000), (0, 0)]\n",
        "co_weighteds                    = [0]\n",
        "weight_modes                    = ['softmax']\n",
        "use_c_u_ratios                  = [False, True]\n",
        "\n",
        "plot_pixel_acts   = True\n",
        "plot_pair_ratios  = True\n",
        "id_dot, ood_dot   = (850, 1900, 'lime'), (485, 1360, 'red') # Place these manually by choosing coordinates on a ood object and an id object.\n",
        "\n",
        "state = calib.state_dict()\n",
        "\n",
        "\n",
        "grid = {\n",
        "    \"temp_co\":                temperatures_co,\n",
        "    \"activation_clipping\":    activation_clippings,\n",
        "    \"activation_pruning\":     activation_prunings,\n",
        "    \"weight_pruning\":         weight_prunings,\n",
        "    \"wt_pair\":                list(zip(wt_thresholds, wt_u_thresholds)),\n",
        "    \"co_blur_pair\":           list(zip(co_blur_sizes, co_sigmas)),\n",
        "    \"clip_val\":               clips,\n",
        "    \"temp_line\":              temperatures_line,\n",
        "    \"co_weighted\":            co_weighteds,\n",
        "    \"inv2ones\":               inverse_convert_to_ones,\n",
        "    \"bin_or_not\":             binary_or_not,\n",
        "    \"baseline_pair\":          list(zip(baseline_and_line_blur_ksizes, baseline_and_line_sigmas)),\n",
        "    \"max_pool\":               max_pools,\n",
        "    \"weight_mode\":            weight_modes,\n",
        "    \"c_u_ratio\":              use_c_u_ratios,\n",
        "}\n",
        "\n",
        "param_names, param_values = zip(*grid.items())\n",
        "\n",
        "# 4. Tracking best metrics\n",
        "best_ap    =  -1\n",
        "best_fpr   =   1\n",
        "best_auc   =  -1\n",
        "best_tag_ap  = best_tag_fpr = best_tag_auc = None\n",
        "\n",
        "# 5. Iterate over the Cartesian product of all hyper-params\n",
        "for combo in itertools.product(*param_values):\n",
        "    P = dict(zip(param_names, combo))\n",
        "\n",
        "    # unpack zipped pairs\n",
        "    wt_thr,  wt_thr_u      = P[\"wt_pair\"]\n",
        "    co_blur_ksize, co_sigma = P[\"co_blur_pair\"]\n",
        "    baseline_blur, baseline_sigma = P[\"baseline_pair\"]\n",
        "\n",
        "    # create & configure detector\n",
        "    det = AnomalyDetector(matched_images_short,\n",
        "                          matched_annotations_short,\n",
        "                          model,\n",
        "                          state)\n",
        "    det.plot_pixel_activations = plot_pixel_acts\n",
        "    det.plot_pair_ratios       = plot_pair_ratios\n",
        "    det.images_to_be_plotted   = images_to_be_plotted\n",
        "    det.id_dot, det.ood_dot    = id_dot, ood_dot\n",
        "\n",
        "    # set thresholds & params\n",
        "    det.activation_clipping     = P[\"activation_clipping\"]\n",
        "    det.activation_pruning      = P[\"activation_pruning\"]\n",
        "    det.weight_pruning          = P[\"weight_pruning\"]\n",
        "    det.temperature_co          = P[\"temp_co\"]\n",
        "    det.temperature_line        = P[\"temp_line\"]\n",
        "    det.inverse_convert_to_ones = P[\"inv2ones\"]\n",
        "    det.make_feat_binary        = P[\"bin_or_not\"]\n",
        "    det.wt_threshold            = wt_thr\n",
        "    det.wt_u_threshold          = wt_thr_u\n",
        "    det.co_blur_ksize           = co_blur_ksize\n",
        "    det.co_sigma                = co_sigma\n",
        "    det.blur_ksize              = baseline_blur\n",
        "    det.sigma                   = baseline_sigma\n",
        "    det.max_pool                = P[\"max_pool\"]\n",
        "    det.clips                   = P[\"clip_val\"]\n",
        "    det.co_weighted             = P[\"co_weighted\"]\n",
        "    det.weight_mode             = P[\"weight_mode\"]\n",
        "    det.c_u_ratio               = P[\"c_u_ratio\"]\n",
        "\n",
        "    # run inference\n",
        "    det.ood_inference()\n",
        "    IDs, ood_scores = det.get_ood_score_lists()\n",
        "\n",
        "    # evaluate selected methods\n",
        "    methods = {\n",
        "        \"ood_scores_coact_bin\":       \"Co-act count\",\n",
        "        \"ood_scores_coact_wt\":        \"Co-act magnitude\",\n",
        "        \"ood_scores_coact_any\":       \"Co-act any\",\n",
        "        \"ood_scores_nr_activation\":   \"#activations\",\n",
        "    }\n",
        "\n",
        "    # build a reusable run tag\n",
        "    base_tag = \" | \".join(f\"{k}={P[k]}\" for k in param_names)\n",
        "\n",
        "    scores_line = ood_scores[\"ood_scores_line\"]\n",
        "    for method, title in methods.items():\n",
        "        scores = -ood_scores[method] / scores_line / scores_line / scores_line\n",
        "        scores = (scores - scores.min()) / (scores.ptp() + 1e-9)\n",
        "\n",
        "        run_tag = f\"method | ={method}{base_tag}\"\n",
        "        print(run_tag)\n",
        "\n",
        "        auc, fpr, ap = det.calculate_metrics(IDs, scores,\n",
        "                                             plot_hist=False,\n",
        "                                             title=title)\n",
        "\n",
        "        if ap  > best_ap:  best_ap,  best_tag_ap  = ap,  run_tag\n",
        "        if fpr < best_fpr: best_fpr, best_tag_fpr = fpr, run_tag\n",
        "        if auc  > best_auc: best_auc, best_tag_auc = auc, run_tag\n",
        "\n",
        "    # cleanup GPU memory\n",
        "    del det\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 6. Print summary of best combinations\n",
        "print(\"\\n══════ BEST RESULTS ══════\")\n",
        "print(f\"• Best AP    = {best_ap:.4f}  → {best_tag_ap}\")\n",
        "print(f\"• Best FPR95 = {best_fpr:.4f}  → {best_tag_fpr}\")\n",
        "print(f\"• Best AUROC = {best_auc:.4f}  → {best_tag_auc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bC1Pg3Xt_Pt9"
      },
      "outputs": [],
      "source": [
        "# Set true and run if you need to free memory\n",
        "\n",
        "delete = True\n",
        "if delete:\n",
        "    del det\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    import torch\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}